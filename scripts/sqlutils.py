#!/usr/bin/env python3

#
# Copyright (C) 2017-2025 Tactical Computing Laboratories, LLC
# All Rights Reserved
# contact@tactcomplabs.com
#
# See LICENSE in the top level directory for licensing details
#
# sqlutils.py
#

import argparse
import json
import os
import re
import sqlite3
import sys

# __all__ = ['file_info', 'timing_info', 'conf_info', 'slurm_info']

# Example json record generated by `sst --timing-info-json timing.json``
# {"timing-info":{
#     "local_max_rss":132256,              // Max Resident Set Size (kB)         
#     "global_max_rss":5055264,            // Approx. Global Max RSS Size (kB)
#     "local_max_pf":669,                  // Max Local Page Faults 
#     "global_pf":23423,                   // Global Page Faults 
#     "global_max_io_in":6073,             // Max Output Blocks
#     "global_max_io_out":0,               // Max Input Blocks 
#     "global_max_sync_data_size":48432,   // Max Sync data size 
#     "global_sync_data_size":1881250,     // Global Sync data size 
#     "max_mempool_size":11534336,         // Max mempool usage (bytes)
#     "global_mempool_size":461373440,     // Global mempool usage (bytes)
#     "global_active_activities":40,       // Global active activities 
#     "global_current_tv_depth":80,        // Current global TimeVortex depth 
#     "global_max_tv_depth":126,           // Max TimeVortex depth 
#     "max_build_time":0.2318727970123291, // Build time (wallclock seconds)
#     "max_run_time":6.7511022090911865,   // Run loop time (wallclock seconds)
#     "max_total_time":6.986878871917725,  // Total time (wallclock seconds) 
#     "simulated_time_ua":"1 ms"}          // Simulated time ( Algebra seconds string. eg. "10 us" )
# }

jobInfoTable = "job_info"
timingInfoTable = "timing_info"
fileInfoTable = "file_info"
slurmInfoTable = "slurm_info"
confInfoTable = "conf_info"

# all tables have implied 'jobid' as unique primary key
keyDict = {
    jobInfoTable : [
        "friend",    # related simulation ( e.g. base simulation id for checkpoint sim comparison)
        "jobtype",   # BASE, CPT, RST, COMPLETION
        "jobstring", # command line
        "slurm",     # using slurm. If not then local mpi sim
        "cwd",       # location where job was run
        "cpt_num",   # checkpoint number if RST job type
        "cpt_timestamp", # checkpoint timestamp if RST job type
        "nodeclamp",  # sets nodes constant if non-zero
        "jobnodes",   # nodes requests with job submission
    ],
    timingInfoTable : [
        "local_max_rss",
        "global_max_rss",
        "local_max_pf",
        "global_pf",
        "global_max_io_in",
        "global_max_io_out",
        "global_max_sync_data_size",
        "global_sync_data_size",
        "max_mempool_size",
        "global_mempool_size",
        "global_active_activities",
        "global_current_tv_depth",
        "global_max_tv_depth",
        "max_build_time",
        "max_run_time",
        "max_total_time",
        "simulated_time_ua",
        "ranks",
        "threads",
        ],
    fileInfoTable : [
        "jobpath",
        "disk_usage",       # du -d 0 --block-size=1 | awk '{print $1}'
        "cpt_dirs",         # find . -name '[1-9]*_[1-9]*' | wc -l
        "cpt_bin",          # find . -name '*[0-9]*.bin' | wc -l
        "cpt_bin_size_max", # find . -name '*[0-9]*.bin' | xargs ls -lS | head -1 | awk '{print $5}'
        "cpt_bin_size_min", # find . -name '*[0-9]*.bin' | xargs ls -lS | tail -1 | awk '{print $5}'
        "cpt_bin_size_ave", # find . -name '*[0-9]*.bin' | xargs ls -lS | awk 'BEGIN {s=0;n=0} {s+=$5; n++} END { if (n>0) { print s/n; } else { print 0; } }'
        "cpt_bin_size_total", # 
    ],
    slurmInfoTable : [ 
        "slurm_id", "jobname", "user", "elapsed", "cpus", "nodes",
        "maxmem", "maxvmem", "maxpages",
        "minmem", "minvmem", "minpages",
        "avgmem", "avgvmem", "avgpages"
    ],
    # json keys have '-' which need to be converted to '_' for sqlite3
    # This is all based on 2d.py so component0 and link0 give all the 
    # information we need. Maybe this can become more general
    confInfoTable: [
        # summary info
        "components",         # number of components
        "links",              # number of links
        "statisticOutput",    # "sst.statOutputConsole"
        # "program_options" : {...}
        "timebase",               # "1 ps"
        "partitioner",            # "sst.linear"
        "timeVortex",             # "sst.timevortex.priority_queue"
        "checkpoint-sim-period",  # "2000ns"
        "checkpoint-wall-period", # "0"
        #TODO what about other command line options like checkpoint-prefix missing from config.json
        # "components" : {...}
        "comp-name",       # "cp_0_0"
        "comp-type",       # "grid.GridNode"
        "numBytes",   # rest under "params" : {...} specific to 2d.py
        "numPorts",
        "minData",
        "maxData",
        "minDelay",
        "maxDelay",
        "clocks",
        "rngSeed",
        "clockFreq",  # "1GHz"
    ],
}

# Sort the keys and generated sql strings for inserting data
sortedKeyDict = {}
sqlKeyStrings = {}
sqlQStrings = {}
for table in keyDict:
    sortedKeyDict[table] = sorted(keyDict[table])
    sqlKeyStrings[table] = ', '.join([f'{s.replace("-","_")}' for s in sortedKeyDict[table]])
    sqlQStrings[table] = "?, " + ', '.join(['?' for s in sortedKeyDict[table]])

def log_sql_callback(statement):
    print("Executing SQL statement:", statement)

class sqldb():
    
    def __init__(self, dbFile, logging):
        self.con = sqlite3.connect(dbFile)
        if logging==True:
            self.con.set_trace_callback(log_sql_callback)
        self.cur = self.con.cursor()
        for table in keyDict:
            qy = f"CREATE TABLE IF NOT EXISTS {table} (jobid INTEGER PRIMARY KEY, {sqlKeyStrings[table]})"
            self.cur.execute(qy)
        self.con.commit()

    def commit(self):
        self.con.commit()

    def close(self):
        self.con.close()

    def insertFromJSON(self, jobid, jsonFile, jsonKey, tableName):
        with open(jsonFile) as f:
            jsonDict = json.load(f)
        jsonInfo = jsonDict[jsonKey]
        data = ( jobid, )
        for k in sortedKeyDict[tableName]:
            data += ( jsonInfo[k], )
        self.cur.execute(f"INSERT INTO {tableName} VALUES( {sqlQStrings[tableName]})", data)

    def insertRecord(self, jobid, dataDict, tableName):
        data = ( jobid, )
        for k in sortedKeyDict[tableName]:
            data += ( dataDict[k], )
        self.cur.execute(f"INSERT INTO {tableName} VALUES( {sqlQStrings[tableName]})", data)

    # table update subcommands
    def job_info(self, *, jobid:int, dataDict:dict ):
        self.insertRecord(jobid, dataDict, jobInfoTable)
    
    def timing_info(self, *, jsonFile:str=None, jobpath:str, jobid:int):
        if jsonFile == None:
            jsonFile=f"{jobpath}/timing.json"
        self.insertFromJSON(jobid, jsonFile, 'timing-info', timingInfoTable)

    def file_info(self, *, jobpath:str, jobid:int):
        # .../_grid_perf/687804907541/_cpt/1_500000/grid_4_0.bin
        # re_bin=re.compile("^.*_[0-9]+\\.bin$")
        re_bin=re.compile("^.*/([0-9]+)_([0-9]+)/.+[0-9]\\.bin$")
        fileDict = dict.fromkeys(sortedKeyDict[fileInfoTable], 0)
        fileDict['jobpath'] = jobpath
        for dirpath, dirnames, filenames in os.walk(jobpath):
            for filename in filenames:
                file_path = os.path.join(dirpath, filename)
                file_size = os.path.getsize(file_path)
                fileDict['disk_usage'] += file_size
                m=re_bin.match(file_path)
                if m != None:
                    # print(f"{file_path} {file_size}")
                    if fileDict['cpt_bin'] == 0:
                        fileDict['cpt_bin_size_max'] = file_size
                        fileDict['cpt_bin_size_min'] = file_size
                    if file_size > fileDict["cpt_bin_size_max"]:
                        fileDict['cpt_bin_size_max'] = file_size
                    if file_size < fileDict["cpt_bin_size_min"]:
                        fileDict['cpt_bin_size_min'] = file_size
                    fileDict['cpt_bin'] += 1
                    fileDict['cpt_bin_size_total'] += file_size
            # assumes using --checkpoint-prefix='_cpt'
            re_cpt=re.compile(f"^.*/_cpt$")
            for dirname in dirnames:
                m=re_cpt.match(dirpath)
                if m != None:
                    # print(f"{jobid} {dirname} {dirpath}")
                    fileDict['cpt_dirs'] += 1
        # final bit
        if fileDict['cpt_bin'] != 0:
            fileDict['cpt_bin_size_ave'] = fileDict['cpt_bin_size_total'] / fileDict['cpt_bin']
        # update the database
        self.insertRecord(jobid, fileDict, fileInfoTable)

    # slurm-info subcommand
    def slurm_info(self, *, jsonFile:str=None, jobpath:str, jobid:int):
        if jsonFile == None:
            jsonFile=f"{jobpath}/slurm.json"
        slurmObj = SlurmObj(jsonFile)
        self.insertRecord(jobid, slurmObj.dbDict, slurmInfoTable)

    # conf-info subcommand for comp_info table
    # Some of this could be made generic but 
    # currently based on using 2d.py as the SDL
    def conf_info(self, *, jsonFile:str=None, jobpath:str, jobid:int):
        jsonFile=jsonFile
        if jsonFile == None:
            jsonFile=f"{jobpath}/config.json"
        try:
            with open(jsonFile) as f:
                jsonDict = json.load(f)
        except FileNotFoundError:
            # sst simulations loading a checkpoint do not produce conf output
            # print(f"Warning: File not found: {jsonFile}")
            return
        program_options = jsonDict["program_options"]
        links = jsonDict["links"]
        statistics_options=jsonDict["statistics_options"]
        components = jsonDict["components"]
        confinfo = {}
        # summary info
        confinfo["components"] = len(components)
        confinfo["links"] = len(links)
        confinfo["statisticOutput"] = statistics_options["statisticOutput"]
        # program options
        confinfo["timebase"] = program_options["timebase"]
        confinfo["partitioner"] = program_options["partitioner"]
        confinfo["timeVortex"] = program_options["timeVortex"]
        confinfo["checkpoint-sim-period"] = program_options["checkpoint-sim-period"]
        confinfo["checkpoint-wall-period"] = program_options["checkpoint-wall-period"]
        # Flattened component0 (grid.GridNode)
        confinfo["comp-name"] = components[0]["name"]
        confinfo["comp-type"] = components[0]["type"]
        params = components[0]["params"]
        confinfo["numBytes"]  = params["numBytes"]
        confinfo["numPorts"]  = params["numPorts"]
        confinfo["minData"]   = params["minData"]
        confinfo["maxData"]   = params["maxData"]
        confinfo["minDelay"]  = params["minDelay"]
        confinfo["maxDelay"]  = params["maxDelay"]
        confinfo["clocks"]    = params["clocks"]
        confinfo["rngSeed"]   = params["rngSeed"]
        confinfo["clockFreq"] = params["clockFreq"]
        data = ( jobid, )
        for k in sortedKeyDict[confInfoTable]:
            data += ( confinfo[k], )
        self.cur.execute(f"INSERT INTO {confInfoTable} VALUES( {sqlQStrings[confInfoTable]})", data)

class SlurmObj():
        
    def __init__(self, filename):
        self.filename = filename
        with open(filename) as f:
            self.jdict = json.load(f)
        # Top level: meta{}, errors[], jobs[]
        self.meta = self.jdict['meta']
        # print(f"{self.filename} Slurm v{self.meta['Slurm']['release']} {self.meta['plugin']['type']} {self.meta['plugin']['name']}")
        errors = self.jdict['errors']
        jobs = self.jdict['jobs']
        if (len(errors) > 0):
            print(f"error: {filename} has errors {errors}")
            sys.exit(1)
        # should be one and only one job
        if len(jobs) != 1:
            print(f"error: expected 1 job in {filename}. Found {len(jobs)}")
            sys.exit(1)
        # flatten data and associate with informative keys
        jobDict = jobs[0]
        self.dbDict = dict.fromkeys(sortedKeyDict[slurmInfoTable], 0)
        self.dbDict = self.flattenData(jobDict)

    def flattenData(self, jDict):
        d = {}
        d['slurm_id'] = jDict['job_id']
        d['jobname'] = jDict['name']
        d['user'] = jDict['user']
        d['elapsed'] = jDict['time']['elapsed']
        d['cpus'] = jDict['required']['CPUs']
        if jDict['state']['current'] != "COMPLETED":
            print(f"error: job {d['slurm_id']} state is {jDict['state']['current']}")
            sys.exit(1)
        for t in jDict['tres']['allocated']:
            if t['type'] == "node":
                d['nodes'] = t['count']
        d['maxmem'] = 0
        d['maxvmem'] = 0
        d['maxpages'] = 0
        for s in jDict['steps']:    
            for t in s['tres']['requested']['max']:
                if t['type'] == "mem":
                    d['maxmem'] += t['count']
                elif t['type'] == "vmem":
                    d['maxvmem'] += t['count']
                elif t['type'] == 'pages':
                    d['maxpages'] += t['count']
        d['minmem'] = 0
        d['minvmem'] = 0
        d['minpages'] = 0
        for s in jDict['steps']:    
            for t in s['tres']['requested']['min']:
                if t['type'] == "mem":
                    d['minmem'] += t['count']
                elif t['type'] == "vmem":
                    d['minvmem'] += t['count']
                elif t['type'] == 'pages':
                    d['minpages'] += t['count']  
        d['avgmem'] = 0
        d['avgvmem'] = 0
        d['avgpages'] = 0
        for s in jDict['steps']:    
            for t in s['tres']['requested']['average']:
                if t['type'] == "mem":
                    d['avgmem'] += t['count']
                elif t['type'] == "vmem":
                    d['avgvmem'] += t['count']
                elif t['type'] == 'pages':
                    d['avgpages'] += t['count']  
        return d

# subcommand wrappers
def _timing_info(db:sqldb, args):
    db.timing_info(jsonFile=args.jsonFile, jobpath=args.jobpath, jobid=args.jobid)

def _file_info(db, args):
    db.file_info(jobpath=args.jobpath, jobid=args.jobid)

def _slurm_info(db, args):
    db.slurm_info(jsonFile=args.jsonFile, jobpath=args.jobpath, jobid=args.jobid)

def _conf_info(db, args):
    db.conf_info(jsonFile=args.jsonFile, jobpath=args.jobpath, jobid=args.jobid)

if __name__ == '__main__':

    # main parser
    parser = argparse.ArgumentParser(
        prog="sqlutils.py",
        description="Utilies for extracting run information from sst jobs run over slurm")
    # common args
    parent_parser = argparse.ArgumentParser(add_help=False)
    # parent_parser.add_argument("--jobtype", type=str, help="type of job run [None]")
    parent_parser.add_argument("--db", type=str, default="timing.db", help="sqlite database file to be created or updated [timing.db]")
    parent_parser.add_argument("--logging", action="store_true", help="print debug logging messages")
    parent_parser.add_argument("--jobid",   type=int, required=True, help="unique job identifier")
    parent_parser.add_argument("--jobpath", type=str, required=True, help="path to run directory")
    # sub-parsers
    subparsers = parser.add_subparsers(title="subcommands", dest="subcommand", help='available subcommands. Use {subcommand --help} for more detail')
    # timing_info table                   
    parser_json = subparsers.add_parser(
        'timing-info', 
        help='update timing_info table using json file generated with "sst --timing-info-json"',
        parents=[parent_parser])
    parser_json.set_defaults(func=_timing_info)
    parser_json.add_argument("--jsonFile", type=str, help="name of JSON file [{jobpath}/timing.json]")
    # file_info table
    parser_file_info = subparsers.add_parser(
        'file-info', 
        help='update file-info table',
        parents=[parent_parser])
    parser_file_info.set_defaults(func=_file_info)
    # slurm_info table
    parser_slurm_info = subparsers.add_parser(
        'slurm-info', 
        help='update slurm-info table using json file generated with "sacct -l -j ${{SLURM_JOB_ID}} --json"',
        parents=[parent_parser])
    parser_slurm_info.set_defaults(func=_slurm_info)
    parser_slurm_info.add_argument("--jsonFile", type=str, help="name of JSON file [{jobpath}/slurm.json]")
    # conf_info table using conf.json
    parser_conf_info = subparsers.add_parser(
        'conf-info', 
        help='update conf-info table from --output-json=conf.json',
        parents=[parent_parser])
    parser_conf_info.set_defaults(func=_conf_info)
    parser_conf_info.add_argument("--jsonFile", type=str, help="name of JSON file [{jobpath}/config.json]")

    # validate user input
    args = parser.parse_args()
    if hasattr(args, 'func') == False:
        parser.print_help()
        sys.exit(1)
    if args.logging:
        for arg in vars(args):
            print("\t", arg, " = ", getattr(args, arg))

    # Create or open database file
    db = sqldb(args.db, args.logging)

    # Invoke selection
    args.func(db, args)

    db.commit()
    db.close()
    
